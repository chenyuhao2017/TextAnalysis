# (三)基于深度学习方法的情感分析 
（基于keras框架）
tensorflow安装1.13.1版本（tensorflow==1.13.1），否则版本过高将导致无法与keras兼容
由于tensorflow不支持python3.6以上版本，在安装之前先自行查看python版本，需降至3.6及以下

### 模型训练阶段：
### 3.1语料库预处理
#### 3.1.1语料库标签化tokensize  
# 文本数据处理（转变为rnn模型可用格式）
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
# 词向量长度——每个词都对应一个长度为200的向量
embedding_dim=len(w2v["很"])
embedding_dim
#(题外)计算两个词的相似度
w2v.similarity("满意","喜欢")
#(题外)找出最相近的词,利用余弦相似度
w2v.most_similar(["贵"],topn=10)
# 训练语料pos[0]&neg[0] 含有8161条正面评论，8154条负面评论
# 前面已经进行过分词与合并语料 x
#len(x)
texts_org=x.tolist()
# 将评论索引化tokenize：将评论中的词转换为word2vec中的索引index
texts_tokens=[]
for text in texts_org:
    for i,word in enumerate(text):
        try:
            text[i]=w2v.wv.vocab[word].index
        except KeyError:
            text[i]=0
    texts_tokens.append(text)
# 获得所有tokens的长度
num_tokens = [len(tokens) for tokens in texts_tokens]
num_tokens = np.array(num_tokens)
# num_tokens
# 计算平均tokens的长度/最长评价的长度
# np.mean(num_tokens)
np.max(num_tokens)
# 图形展示——tokens长度的分布情况
import matplotlib.pyplot as plt
plt.hist(num_tokens,bins = 100,color = "royalblue")
plt.xlim((0,400))
plt.ylabel("num of tokens")
plt.xlabel("len of tokens")
plt.title("distribution of tokens length")
plt.show()
# 正态分布的时候根据3-Sigma，计算平均值加二倍标准差，预计可覆盖95%以上的范围(98)
max_tokens = int(np.mean(num_tokens) + 2 * np.std(num_tokens))
# max_tokens
np.sum(num_tokens < max_tokens) / len(num_tokens)
# 填充和修剪 padding & truncation 以保持所有tokens的长度相等=98
# 过多会浪费计算资源，过少会影响模型效果
texts_pad = pad_sequences(texts_tokens , maxlen = max_tokens,
                          padding = "pre", truncating = "pre")
# 超过字典长度的词用0代替（选用的词中没有）
texts_pad[texts_pad >= len(w2v.wv.vocab)] = 0
#### 3.1.2 构建Embedding Matrix
keras要求准备一个维度为（num_words,embedding_dim)的矩阵，num_words表示使用的词汇数量（使用的词典中的词汇的数量），embedding_dim表示维度，即每一个词语都用一个长度为embedding_dim的向量表示
# 矩阵初始化（1681*200）
embedding_matrix = np.zeros((num_words, embedding_dim))
# 矩阵赋值得到词嵌入矩阵
for i in range(num_words):
    embedding_matrix[i,:] = w2v[w2v.wv.index2word[i]]
embedding_matrix = embedding_matrix.astype("float32")
# 检验是否词嵌入矩阵是否与词嵌入模型的向量一一对应
# 返回true的个数
np.sum(w2v[w2v.wv.index2word[444]] == embedding_matrix[444])
embedding_matrix.shape

####  3.1.3 训练集与测试集构建
# 情感标签y：前8161个为1，后8154个为0
from sklearn.model_selection import train_test_split
# 分割训练集和测试集合,90-10
x_train,x_test,y_train,y_test = train_test_split(texts_pad,y,
                                                test_size=0.1,
                                                random_state=12)
### 3.2 RNN模型构建和训练
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import SimpleRNN
from keras.layers import LSTM
# 构建rnn模型
rnn_model = Sequential()
# 1.词嵌入层
rnn_model.add(Embedding(num_words, embedding_dim,
                    weights = [embedding_matrix],
                   input_length = max_tokens,
                   trainable = False))
rnn_model.add(Dropout(0.35))
# 2.一层RNN
rnn_model.add(SimpleRNN(32))
rnn_model.add(Dropout(0.35))
# 3.全连接层
rnn_model.add(Dense(1, activation = 'sigmoid'))

rnn_model.compile(optimizer = "rmsprop", loss = "binary_crossentropy",metrics = ['acc'])
rnn_model.summary()
history = rnn_model.fit(x_train, y_train, batch_size = 200, epochs = 10, validation_data = (x_test, y_test))
# 直观展示——准确率和损失图
acc = history.history["acc"]
val_acc = history.history["val_acc"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]
# 迭代次数
epochs = range(1, len(acc) + 1)

plt.figure(figsize = (20,5))

# 准确率
plt.subplot(1,2,1)
plt.plot(epochs, acc, "b", label = "Training Acc",color = "grey")
plt.plot(epochs, val_acc, "b", label = "Test Acc",color = "royalblue")
plt.title("Training and Test Accuracy")
plt.legend(loc = 4)

# 损失
plt.subplot(1,2,2)
plt.plot(epochs, loss, "b", label = "Training Loss",color = "grey")
plt.plot(epochs, val_loss, "b", label = "Test Loss",color = "royalblue")
plt.title("Training and Test Loss")
plt.legend(loc = 1)
plt.show()

### 3.3 模型预测
# 导入需预测的评论
# 导入经过预处理的分词结果
texts_predict = []
for comment in comments["content"]:
    words = comment.split(" ")
    texts_predict.append(words)
# 将评论索引化tokenize：将评论中的词转换为word2vec中的索引index
texts_predict_tokens = []
for text in texts_predict:
    for i,word in enumerate(text):
        try:
            text[i] = w2v.wv.vocab[word].index
        except KeyError:
            text[i] = 0
    texts_predict_tokens.append(text)
# padding & truncation
texts_predict_pad = pad_sequences(texts_predict_tokens , maxlen=98,
                          padding = "pre", truncating = "pre")
# 超过字典长度的词用0代替（选用的词中没有）
texts_predict_pad[texts_predict_pad >= len(w2v.wv.vocab)] = 0

# 预测
predict_results = rnn_model.predict_classes(texts_predict_pad)
comments["rnn_results"] = predict_results
# 将1/0结果转换为积极消极
rnn_senti = []
for result in comments["rnn_results"]:
    if result == 1:
        rnn_senti.append("积极")
    else:
        rnn_senti.append("消极")
comments["rnn_results"] = rnn_senti
senti_num("RNN",comments["rnn_results"])
